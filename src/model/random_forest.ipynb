{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e0cf12",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\array\\__init__.py:4\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backends, fft, lib, linalg, ma, overlap, random\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblockwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m atop, blockwise\n",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\array\\backends.py:8\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chunk\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Array\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     concatenate_lookup,\n\u001b[0;32m     11\u001b[0m     divide_lookup,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     to_numpy_dispatch,\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\array\\core.py:56\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     DaskMethodsMixin,\n\u001b[0;32m     49\u001b[0m     compute_as_if_collection,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     tokenize,\n\u001b[0;32m     55\u001b[0m )\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblockwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m blockwise \u001b[38;5;28;01mas\u001b[39;00m core_blockwise\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblockwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m broadcast_dimensions\n",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\blockwise.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhighlevelgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HighLevelGraph, Layer\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SubgraphCallable, fuse\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Graph, Key\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SubgraphCallable' from 'dask.optimization' (c:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\optimization.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\dataframe\\__init__.py:81\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backends, dispatch, rolling\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     83\u001b[0m     DataFrame,\n\u001b[0;32m     84\u001b[0m     Index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m     to_timedelta,\n\u001b[0;32m     91\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\dataframe\\backends.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar, union_categoricals\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Array\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m percentile_lookup\n",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\array\\__init__.py:273\u001b[0m\n\u001b[0;32m    267\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDask array requirements are not installed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either conda or pip install as follows:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  conda install dask                 # either conda install\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  python -m pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdask[array]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m --upgrade  # or python -m pip install\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    272\u001b[0m )\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SubgraphCallable' from 'dask.optimization' (c:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\optimization.py)\n\nDask array requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                 # either conda install\n  python -m pip install \"dask[array]\" --upgrade  # or python -m pip install",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspatialdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_zarr\n",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\spatialdata\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\n\u001b[0;32m      5\u001b[0m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mset({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataframe.query-planning\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Setting `dataframe.query-planning` to False is effective only if run before `dask.dataframe` is initialized. In\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# the case in which the user had initilized `dask.dataframe` before, we would have DASK_EXPER_ENABLED set to `True`.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Here we check that this does not happen.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(dd, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDASK_EXPR_ENABLED\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dd\u001b[38;5;241m.\u001b[39mDASK_EXPR_ENABLED:\n",
      "File \u001b[1;32mc:\\Users\\Mattias\\anaconda3\\envs\\classifier_env\\lib\\site-packages\\dask\\dataframe\\__init__.py:141\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    135\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDask dataframe requirements are not installed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease either conda or pip install as follows:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  conda install dask                     # either conda install\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  python -m pip install \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdask[dataframe]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m --upgrade  # or python -m pip install\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    140\u001b[0m     )\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from spatialdata import read_zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Feature_extractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, functions, to_flatten=False):\n",
    "        super().__init__()\n",
    "        self.to_flatten = to_flatten\n",
    "        self.functions = functions\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.max_height = max(img.shape[0] for img in X)\n",
    "        self.max_width = max(img.shape[1] for img in X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        all_features = []\n",
    "\n",
    "        for img in X:\n",
    "            image_features = []\n",
    "\n",
    "            img = img.astype(np.float32).squeeze()\n",
    "            img = (img - img.min()) / (img.max() - img.min())\n",
    "            image_features.append(img)\n",
    "\n",
    "            h, w = img.shape\n",
    "\n",
    "            x_coords, y_coords = np.meshgrid(np.arange(w), np.arange(h))\n",
    "            x_coords = x_coords / self.max_width\n",
    "            y_coords = y_coords / self.max_height\n",
    "            image_features.append(x_coords)\n",
    "            image_features.append(y_coords)\n",
    "\n",
    "            for f, params in self.functions.items():\n",
    "                image_features.append(f(img, **params))\n",
    "\n",
    "            if self.to_flatten:\n",
    "                image_features = [i.flatten() for i in image_features]\n",
    "                all_features.append(np.stack(image_features, axis=1))\n",
    "            else:\n",
    "                all_features.append(np.stack(image_features))\n",
    "\n",
    "        return np.array(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class Custom_Random_Forest(RandomForestClassifier):\n",
    "    def fit(self, X, y, sample_weight = None):\n",
    "        images, feutures, H, W = X.shape\n",
    "        X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "        X = X.reshape(images*H * W, feutures)\n",
    "        y = y.reshape(images*H * W)\n",
    "\n",
    "        return super().fit(X, y, sample_weight)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        images, feutures, H, W = X.shape\n",
    "        X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "        X = X.reshape(images*H * W, feutures)\n",
    "        pred = super().predict(X)\n",
    "        return pred.reshape(images, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter, gaussian_gradient_magnitude, gaussian_laplace\n",
    "\n",
    "trans = Feature_extractor(functions={\n",
    "    gaussian_filter: {'sigma':1},\n",
    "    gaussian_gradient_magnitude: {'sigma':1},\n",
    "    gaussian_laplace: {'sigma':1},\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb39bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"Feature extractor\", trans),\n",
    "    (\"classifier\", Custom_Random_Forest(n_estimators=50, n_jobs=-1, max_depth=10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac395c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_image_channelwise(image, new_shape=(64, 64), max_channels=30):\n",
    "    C = min(image.shape[-1], max_channels)\n",
    "    resized_channels = []\n",
    "    for c in range(C):\n",
    "        channel = image[:, :, c]\n",
    "        resized = resize(channel, new_shape, preserve_range=True, anti_aliasing=True)\n",
    "        resized_channels.append(resized)\n",
    "    return np.stack(resized_channels, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f8953",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "new_chunks = (512, 512)\n",
    "data_dir = r\"E:\\data\\train\"\n",
    "\n",
    "\n",
    "for arg in os.listdir(data_dir):\n",
    "    path = os.path.join(data_dir, arg)\n",
    "    if os.path.isdir(path) and arg.endswith(\".zarr\"):\n",
    "        sdata = read_zarr(path)\n",
    "\n",
    "        \n",
    "        # for name, image_obj in sdata.images.items():\n",
    "            \n",
    "        data.append(list(sdata.images.values())[0].data.rechunk((1,512,512)))\n",
    "\n",
    "        \n",
    "        ann = sdata[\"annotations\"].data.rechunk(new_chunks)\n",
    "        labels.append(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a944aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "client = Client()\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    pipe.fit(data, labels)\n",
    "    \n",
    "joblib.dump(pipe, \"random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fe57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "data_test = sdata['annotations'].values\n",
    "\n",
    "img = Image.fromarray(data_test)\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifier_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
