{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e0cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from spatialdata import read_zarr\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2122b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import random as r\n",
    "\n",
    "class Feature_extractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, functions, image_size=1024, to_flatten=False):\n",
    "        super().__init__()\n",
    "        self.to_flatten = to_flatten\n",
    "        self.functions = functions\n",
    "        self.max_height = None\n",
    "        self.max_width = None\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Compute max height and width from Dask arrays\n",
    "        heights = [img.shape[0] for img in X]\n",
    "        widths = [img.shape[1] for img in X]\n",
    "        self.max_height = max(heights)\n",
    "        self.max_width = max(widths)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        all_features = []\n",
    "\n",
    "        for img in X:\n",
    "            # Normalize per image - compute min and max\n",
    "            img = img.astype(np.float32).squeeze()\n",
    "            img_min, img_max = da.compute(img.min(), img.max())\n",
    "            img = (img - img_min) / (img_max - img_min + 1e-8)  # Avoid division by zero\n",
    "\n",
    "            h, w = img.shape\n",
    "\n",
    "            x = r.randint(0, w-self.image_size)\n",
    "            y = r.randint(0, h-self.image_size)\n",
    "\n",
    "            # Create normalized coordinates as Dask arrays\n",
    "            x_coords, y_coords = da.meshgrid(da.arange(w), da.arange(h))\n",
    "            x_coords = x_coords[y:y+self.image_size,x:x+self.image_size] / self.max_width\n",
    "            y_coords = y_coords[y:y+self.image_size,x:x+self.image_size] / self.max_height\n",
    "\n",
    "            img = img[y:y+self.image_size,x:x+self.image_size]\n",
    "\n",
    "            image_features = [img, x_coords, y_coords]\n",
    "\n",
    "            # Apply functions: ensure they handle NumPy arrays or compute Dask arrays before applying\n",
    "            for f, params in self.functions.items():\n",
    "                # Compute img to NumPy for function if needed\n",
    "                img_np = img.compute() if isinstance(img, da.Array) else img\n",
    "                feat = f(img_np, **params)\n",
    "                feat_da = da.from_array(feat) if not isinstance(feat, da.Array) else feat\n",
    "                image_features.append(feat_da)\n",
    "\n",
    "            if self.to_flatten:\n",
    "                # Flatten all features and stack as (pixels, features)\n",
    "                flattened = [feat.flatten() if isinstance(feat, da.Array) else feat.ravel() for feat in image_features]\n",
    "                stacked = da.stack(flattened, axis=1).rechunk({1: -1})\n",
    "                all_features.append(stacked)\n",
    "            else:\n",
    "                stacked = da.stack(image_features)  # shape: (num_features, H, W)\n",
    "                all_features.append(stacked)\n",
    "\n",
    "        if self.to_flatten:\n",
    "            # Return list of 2D arrays (pixels x features)\n",
    "            return da.concatenate(all_features, axis=0)\n",
    "        else:\n",
    "            # Return list of 3D arrays (features x H x W)\n",
    "            return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b848379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class Custom_Pipeline(Pipeline):\n",
    "\n",
    "    def predict(self, X):\n",
    "        images, feutures, H, W = X.shape\n",
    "        X = da.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "        X = X.reshape(images*H * W, feutures)\n",
    "        pred = super().predict(X)\n",
    "        return pred.reshape(images, H, W)\n",
    "\n",
    "\n",
    "class Custom_Random_Forest(RandomForestClassifier):\n",
    "    def predict(self, X):\n",
    "        images, feutures, H, W = X.shape\n",
    "        X = da.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "        X = X.reshape(images*H * W, feutures)\n",
    "        pred = super().predict(X)\n",
    "        return pred.reshape(images, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda4690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter, gaussian_gradient_magnitude, gaussian_laplace\n",
    "\n",
    "trans = Feature_extractor(functions={\n",
    "    gaussian_filter: {'sigma':1},\n",
    "    gaussian_gradient_magnitude: {'sigma':1},\n",
    "    gaussian_laplace: {'sigma':1},\n",
    "}, to_flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c0ec8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = r\"E:\\data\\train\"\n",
    "# max_width = 0\n",
    "# max_height = 0\n",
    "# for arg in os.listdir(data_dir):\n",
    "#     path = os.path.join(data_dir, arg)\n",
    "#     if os.path.isdir(path) and arg.endswith(\".zarr\"):\n",
    "#         sdata = read_zarr(path)\n",
    "\n",
    "#         img = sdata['annotations'].data\n",
    "\n",
    "#         h, w = img.shape\n",
    "\n",
    "#         if max_width<w:\n",
    "#             max_width = w\n",
    "\n",
    "#         if max_height<h:\n",
    "#             max_height = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb49db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = {\n",
    "    gaussian_filter: {'sigma':1},\n",
    "    gaussian_gradient_magnitude: {'sigma':1},\n",
    "    gaussian_laplace: {'sigma':1},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6dbab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(img, label, functions, image_size=1024):\n",
    "\n",
    "    h, w = img.shape\n",
    "    x = r.randint(0, w - image_size)\n",
    "    y = r.randint(0, h - image_size)\n",
    "\n",
    "    img = img.astype(np.float32).squeeze()\n",
    "    img_min, img_max = da.compute(img.min(), img.max())\n",
    "    img = (img - img_min) / (img_max - img_min + 1e-8)\n",
    "\n",
    "    # Positional encodings\n",
    "    x_coords, y_coords = da.meshgrid(da.arange(w), da.arange(h))\n",
    "    x_coords = x_coords[y:y+image_size, x:x+image_size] / max_width\n",
    "    y_coords = y_coords[y:y+image_size, x:x+image_size] / max_height\n",
    "\n",
    "    # Crop image\n",
    "    img = img[y:y+image_size, x:x+image_size]\n",
    "\n",
    "    image_features = [img, x_coords, y_coords]\n",
    "\n",
    "    for f, params in functions.items():\n",
    "        # Compute img to NumPy for function if needed\n",
    "        img_np = img.compute() if isinstance(img, da.Array) else img\n",
    "        feat = f(img_np, **params)\n",
    "        feat_da = da.from_array(feat) if not isinstance(feat, da.Array) else feat\n",
    "        image_features.append(feat_da)\n",
    "\n",
    "    return da.array(image_features), label[y:y+image_size, x:x+image_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8eb26d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spatialdata import SpatialData\n",
    "# from spatialdata.models import Image2DModel, Labels2DModel\n",
    "\n",
    "# new_sdata = SpatialData()\n",
    "\n",
    "# count = 0\n",
    "# data_dir = r\"E:\\data\\train\"\n",
    "\n",
    "# for arg in os.listdir(data_dir):\n",
    "#     path = os.path.join(data_dir, arg)\n",
    "#     if os.path.isdir(path) and arg.endswith(\".zarr\"):\n",
    "#         sdata = read_zarr(path)  # Ensure this is defined elsewhere\n",
    "\n",
    "#         label = sdata['annotations'].data\n",
    "\n",
    "#         data = [i.data.squeeze() for i in list(sdata.images.values())]\n",
    "\n",
    "#         for img in data:\n",
    "#             image, l = get_features(img, label, functions)\n",
    "\n",
    "#             new_sdata[f\"channel_{count}\"] = Image2DModel.parse(\n",
    "#             image,\n",
    "#             )\n",
    "\n",
    "#             new_sdata[f\"label_{count}\"] = Labels2DModel.parse(\n",
    "#             l,\n",
    "#             )\n",
    "#             count += 1\n",
    "\n",
    "# new_sdata.write(os.path.join(r\"E:\\data\", \"preprocess.zarr\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef31a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata = read_zarr(r\"E:\\data\\preprocess.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e1928",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "random_data = r.sample(range(len(sdata.labels)), 200)\n",
    "\n",
    "for i in random_data:\n",
    "    flattened = [i[..., 0:512, 0:512].flatten() for i in sdata[f\"channel_{i}\"].values]\n",
    "    stacked = np.stack(flattened, axis=1).astype(np.float16)\n",
    "    X.append(stacked)\n",
    "    y.append(sdata[f\"label_{i}\"].values[0:512, 0:512].flatten()//255)\n",
    "\n",
    "X = np.concatenate(X)\n",
    "y = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d6bac32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52428800, 6)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2dd802fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mattias\\anaconda3\\envs\\tf\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 52555 instead\n",
      "  warnings.warn(\n",
      "2025-05-29 15:17:44,548 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:52584'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HET.pkl']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "import joblib\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, max_depth=10, min_samples_leaf=5, n_jobs=-1)\n",
    "\n",
    "# with joblib.parallel_backend(\"dask\"):\n",
    "model.fit(X, y)\n",
    "\n",
    "joblib.dump(model, \"HET.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "93b8cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = [num for num in range(len(sdata.labels)) if num not in random_data]\n",
    "\n",
    "random_test = r.sample(pool, 100)\n",
    "\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for i in random_test:\n",
    "    flattened = [i.flatten() for i in sdata[f\"channel_{i}\"].values]\n",
    "    stacked = np.stack(flattened, axis=1).astype(np.float16)\n",
    "    test_data.append(stacked)\n",
    "    test_labels.append(sdata[f\"label_{i}\"].values//255)\n",
    "\n",
    "\n",
    "test = np.concatenate(test_data)\n",
    "label = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8c20f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104857600, 6)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1b6fe57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib \n",
    "\n",
    "\n",
    "model = joblib.load(\"HET.pkl\")\n",
    "\n",
    "pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0703009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred.reshape(100, 1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3e132bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_binary(arr1, arr2):\n",
    "    assert arr1.shape == arr2.shape, \"Arrays must have the same shape\"\n",
    "    \n",
    "    intersection = np.logical_and(arr1, arr2).sum()\n",
    "    union = np.logical_or(arr1, arr2).sum()\n",
    "\n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "\n",
    "    score = intersection / union if union != 0 else 0\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a16c6318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float16)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = []\n",
    "\n",
    "for p, l in zip(pred, test_labels):\n",
    "    score = iou_binary(p, l)\n",
    "    mean.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "df737092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3797008837412497"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
